---
output: github_document
---

```{r global_options, include=FALSE}
library(knitr)
opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)
set.seed(123)
```

# featureImportance: Model-agnostic permutation feature importance with the [`mlr`](https://github.com/mlr-org/mlr) package

[![CRAN Status Badge](http://www.r-pkg.org/badges/version/featureImportance)](http://cran.r-project.org/web/packages/featureImportance)
[![CRAN Downloads](http://cranlogs.r-pkg.org/badges/featureImportance)](http://cran.rstudio.com/web/packages/featureImportance/index.html)
[![Build Status](https://travis-ci.org/giuseppec/featureImportance.svg?branch=master)](https://travis-ci.org/giuseppec/featureImportance)
[![codecov](https://codecov.io/gh/giuseppec/featureImportance/branch/master/graph/badge.svg?token=2w8ISxXGMc)](https://codecov.io/gh/giuseppec/featureImportance)

## Results of the article ["Visualizing the Feature Importance for Black Box Models"](https://arxiv.org/abs/1804.06620)

This R package was developed as a part of the article ["Visualizing the Feature Importance for Black Box Models"](https://arxiv.org/abs/1804.06620) accepted at the ECML-PKDD 2018 conference track.
The results of the application section of this article can be reproduced with the code provided [here](https://github.com/giuseppec/featureImportance/blob/master/ecml-demo/application_results.md).

## Installation of the package

Install the development version from GitHub (using `devtools`)

```{r, results = 'hide', eval = FALSE}
install.packages("devtools")
devtools::install_github("giuseppec/featureImportance")
```

## Introduction

The `featureImportance` package is an extension for the [`mlr`](https://github.com/mlr-org/mlr) package and allows to compute the permutation feature importance in a model-agnostic manner.
The focus is on performance-based feature importance measures:

- **Model reliance** and **algorithm reliance**, which is a model-agnostic version of [breiman's permutation importance](https://www.stat.berkeley.edu/~breiman/randomforest2001.pdf) introduced in the article [arXiv:1801.01489v3](https://arxiv.org/abs/1801.01489).
- **SFIMP** (Shapley Feature Importance)
- PIMP

## Use case: Compute importance based on test data

This use case computes the feature importance of a model based on a single test data set.

```{r}
library(mlr)
library(mlbench)
library(featureImportance)
set.seed(2018)

# Get boston housing data and look at the data
data(BostonHousing, package = "mlbench")
str(BostonHousing)

# Create regression task for mlr
boston.task = makeRegrTask(data = BostonHousing, target = "medv")
boston.task

# Specify the machine learning algorithm with the mlr package
lrn = makeLearner("regr.randomForest", ntree = 100)

# Create indices for train and test data
n = getTaskSize(boston.task)
train.ind = sample(n, size = 0.6*n)
test.ind = setdiff(1:n, train.ind)

# Fit model on train data
mod = train(lrn, boston.task, subset = train.ind)

# Use feature values of randomly chosen observations from test data to plot the importance curves
test = getTaskData(boston.task, subset = test.ind)
obs.id = sample(1:nrow(test), 5)

# Measure feature importance on test data
imp = featureImportance(mod, data = test, replace.ids = obs.id, local = TRUE)
summary(imp)

# Plot PI curve
plotImportance(imp, feat = "lstat", mid = "mse", individual = FALSE, hline = TRUE)

# Plot ICI curves
plotImportance(imp, feat = "lstat", mid = "mse", individual = TRUE, hline = FALSE)
```

## Use case: Compute importance using a resampling technique

Instead of computing the feature importance of a model by using a single test data set, one can repeat this process by embedding the feature importance calculation within a resampling procedure.
The resampling procedure creates multiple models using different training sets, and the corresponding test sets can be used to calculate the feature importance.
For example, using 5-fold cross-validation results in 5 different models, one for each cross-validation fold.

```{r}
rdesc = makeResampleDesc("CV", iter = 5)
res = resample(lrn, boston.task, resampling = rdesc, models = TRUE)
imp = featureImportance(res, data = getTaskData(boston.task), n.feat.perm = 10)
summary(imp)
```

