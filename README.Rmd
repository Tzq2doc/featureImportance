---
output: github_document
---

```{r global_options, include=FALSE}
library(knitr)
opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)
set.seed(123)
```

# featureImportance: Model-agnostic permutation feature importance with the [`mlr`](https://github.com/mlr-org/mlr) package

[![CRAN Status Badge](http://www.r-pkg.org/badges/version/featureImportance)](http://cran.r-project.org/web/packages/featureImportance)
[![CRAN Downloads](http://cranlogs.r-pkg.org/badges/featureImportance)](http://cran.rstudio.com/web/packages/featureImportance/index.html)
[![Build Status](https://travis-ci.org/giuseppec/featureImportance.svg?branch=master)](https://travis-ci.org/giuseppec/featureImportance)
[![codecov](https://codecov.io/gh/giuseppec/featureImportance/branch/master/graph/badge.svg?token=2w8ISxXGMc)](https://codecov.io/gh/giuseppec/featureImportance)

## Results of the article ["Visualizing the Feature Importance for Black Box Models"](https://arxiv.org/abs/1804.06620)

This R package was developed as a part of the article ["Visualizing the Feature Importance for Black Box Models"](https://arxiv.org/abs/1804.06620) accepted at the ECML-PKDD 2018 conference track.
The results of the application section of this article can be reproduced with the code provided [here](https://github.com/giuseppec/featureImportance/blob/master/ecml-demo/application_results.md).

## Installation of the package

Install the development version from GitHub (using `devtools`)

```{r, results = 'hide', eval = FALSE}
install.packages("devtools")
devtools::install_github("giuseppec/featureImportance")
```

## Introduction

The `featureImportance` package is an extension for the [`mlr`](https://github.com/mlr-org/mlr) package and allows to compute the permutation feature importance in a model-agnostic manner.
The focus is on performance-based feature importance measures:

- **Model reliance** and **algorithm reliance**, which is a model-agnostic version of [breiman's permutation importance](https://www.stat.berkeley.edu/~breiman/randomforest2001.pdf) introduced in the article [arXiv:1801.01489v3](https://arxiv.org/abs/1801.01489).
- **SFIMP** (Shapley Feature Importance)
- PIMP

## Use case: Compute importance based on test data

This use case computes the feature importance of a model based on a single test data set.
For this purpose, we first build a model (here a random forest) on training data:

```{r}
library(mlr)
library(mlbench)
library(ggplot2)
library(gridExtra)
library(featureImportance)
set.seed(2018)

# Get boston housing data and look at the data
data(BostonHousing, package = "mlbench")
str(BostonHousing)

# Create regression task for mlr
boston.task = makeRegrTask(data = BostonHousing, target = "medv")

# Specify the machine learning algorithm with the mlr package
lrn = makeLearner("regr.randomForest", ntree = 100)

# Create indices for train and test data
n = getTaskSize(boston.task)
train.ind = sample(n, size = 0.6*n)
test.ind = setdiff(1:n, train.ind)

# Create test data using test indices
test = getTaskData(boston.task, subset = test.ind)

# Fit model on train data using train indices
mod = train(lrn, boston.task, subset = train.ind)
```

In general, there are two ways how the feature importance can be computed:

1. Using fixed feature values: Here, the feature values are set to fixed values of the observation specified by `replace.ids`.
2. Permuting the feature values: Here, the values of the feature are randomly permuted `n.feat.perm` times.

### Using fixed feature values

Visualizing the feature importance using fixed feature values is analogous to partial dependece plots and has the advantage that the local feature importance is calculated for each observation in the test data at the same feature values:

```{r}
# Use feature values of 20 randomly chosen observations from test data to plot the importance curves
obs.id = sample(1:nrow(test), 20)

# Measure feature importance on test data
imp = featureImportance(mod, data = test, replace.ids = obs.id, local = TRUE)
summary(imp)

# Plot PI and ICI curves for the lstat feature
pi.curve = plotImportance(imp, feat = "lstat", mid = "mse", individual = FALSE, hline = TRUE)
ici.curves = plotImportance(imp, feat = "lstat", mid = "mse", individual = TRUE, hline = FALSE)
grid.arrange(pi.curve, ici.curves, nrow = 1)
```

### Permuting the feature

Instead of using fixed feature values, the feature importance can also be computed by permuting the feature values. 
Here, the PI curve and ICI curves are evaluated on different randomly selected feature values.
Thus, a smoother is internally used for plotting the curve:

```{r}
# Measure feature importance on test data
imp = featureImportance(mod, data = test, n.feat.perm = 20, local = TRUE)
summary(imp)

# Plot PI and ICI curves for the lstat feature
pi.curve = plotImportance(imp, feat = "lstat", mid = "mse", individual = FALSE, hline = TRUE)
ici.curves = plotImportance(imp, feat = "lstat", mid = "mse", individual = TRUE, hline = FALSE)
grid.arrange(pi.curve, ici.curves, nrow = 1)
```

## Use case: Compute importance using a resampling technique

Instead of computing the feature importance of a model based on a single test data set, one can repeat this process by embedding the feature importance calculation within a resampling procedure.
The resampling procedure creates multiple models using different training sets, and the corresponding test sets can be used to calculate the feature importance.
For example, using 5-fold cross-validation results in 5 different models, one for each cross-validation fold.

```{r}
rdesc = makeResampleDesc("CV", iter = 5)
res = resample(lrn, boston.task, resampling = rdesc, models = TRUE)
imp = featureImportance(res, data = getTaskData(boston.task), n.feat.perm = 20, local = TRUE)
summary(imp)
plotImportance(imp, feat = "lstat", mid = "mse", individual = FALSE, hline = TRUE)
```
